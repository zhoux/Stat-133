{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is a reminder on basics probability notion, adapted from [Elements of Classical Statistics](http://link.springer.com/book/10.1007/3-540-29288-8), by J. W. F. Kardaum (available for free from campus). \n",
      "\n",
      "The definitions are more formal that that of your other books, allowing for a very compact summary of the whole theory, and a bird-eye view on the whole theory. Try to understand the formal defitions, and see that they encopmass those of Rice, or Pitman. \n",
      "\n",
      "Working on this reminder will force your probability and set theory muscle to wake and warm up for what comes next."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Probability measures and probability spaces"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Definition:** A *probability space* is defined by a tripble $(\\Omega, \\mathcal{B}(\\Omega), P)$, where $\\Omega$ is a set, $\\mathcal{B}(\\Omega)$ is a $\\sigma$-algebra of subsets of $\\Omega$, and $P$ is a probability measure.\n",
      "\n",
      "**Interpretation:** The elements of $\\mathcal B(\\Omega)$, denoted by $A$, $B$, $\\dots$  are interpreted as *events*. For example, $\\Omega = \\{1,2,...,6\\}$, $A = \\{4,5,6\\}$, $B = \\{2\\}$. $B$ is called an *elementary event* or *outcome*, and $A$ is a *compound event*. The usual set operations are interpreted accordingly: $A \\cap B$ denotes the (compound) event that event $A$ as well as event $B$ takes place, $A \\cup B$ means that event $A$ or event $B$ takes place (we consistently adhere to the logical use of non-exclusive \u2018or\u2019), $A^c$ that the event $A$ does not occur, $A \\subset B$ that $B$ has to occur if $A$ occurs, $B - A = B \\cap A^c$ that $B$ takes place, but $A$ does not occur. The empty set, $\\varnothing = A \\cap A^c$ is used to indicate a *logically impossible* event, which cannot happen.\n",
      "\n",
      "\n",
      "\n",
      "**Definition:** A familly $\\mathcal{U}$ of subsets of $\\Omega$ is called a $\\sigma$-algebra if it has the following properties:\n",
      "\n",
      "1. $\\varnothing\\in \\mathcal U$\n",
      "1. $A\\in \\mathcal U$ implies $A^c \\in \\mathcal U$\n",
      "1. $A_1, A_1, \\dots \\in \\mathcal U$ implies that $\\cup_{i=1}^{\\infty} A_i \\in \\mathcal U$ for any denumerable collection of subsets of $A_i$. \n",
      "\n",
      "\n",
      "**Definition:** A probability measure $P$ is a function $P:\\mathcal B(\\Omega)\\rightarrow [0,1]\\subset \\mathcal R$ satisfying:\n",
      "\n",
      "1. $P(\\varnothing) =0$ and $P(\\Omega) =1$ (Positivity and normalization) \n",
      "1. $P(A\\cup B) = P(A) + P(B)$ if, and only if, $A\\cap B = \\varnothing $ (Finite additivity)\n",
      "1. $P(\\cup_{i=1}^\\infty A_i) = \\sum_{i=1}^\\infty P(A_i)$ if $A_i\\cap A_j = \\varnothing$ for all $i$ and $j$ with $i\\neq j$ ($sigma$-additivity)\n",
      "\n",
      "**Exercise:** Derive from the axioms in the definition of a probability measure and the usual set operations that\n",
      "\n",
      "1. $P (A^c ) = 1 \u2212 P (A)$,\n",
      "1. $P (A) + P (B-A) = P (A \\cup B)$,\n",
      "1. $P (A \\cap B) + P (B-A) = P (B)$,\n",
      "1. $P (A \\cup B) + P (A \\cap B) = P (A) + P (B)$,\n",
      "1. $A \\subset B$ implies $P (A) \u2264 P (B)$ (monotonicity). Draw Venn diagrams\n",
      "illustrating these properties."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Independent events and conditional probabilities"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Defnition:** Two events $A$ and $B$ are called *independent* if $P(A \\cap B) = P(A)P(B)$.\n",
      "\n",
      "**Definition:** For each $A,B \\in \\mathcal B(\\Omega)$ with $P(B) \\neq 0$, we define $$P(A|B) := P(A \\cap B)/P(B),$$ which is to be interpreted as the probability that event $A$ take place, given that (it is certain that) event $B$ will take place. It is called the *conditional probability* of $A$ given $B$. \n",
      "\n",
      "**Remark:** If $A$ and $B$ are independent, then $P(A|B) = P(A)$. If $B$ implies $A$, i.e. $B \u2282 A$, then $P(A|B) = 1$.\n",
      "\n",
      "**Bayes' Theorem:** Let $(\\Omega, \\mathcal B(\\Omega), P)$ be a probability space and $A_1,\\dots,A_k$ a *partition* of the sample space $\\Omega$ (i.e. $\\Omega = A_1\\cup A_2 \\cup \\dots \\cup A_k$), and let $B$ be any other event. Then\n",
      "$$P(A_i|B) = \\frac{P(B|A_i)P(A_i)}{P(B)}$$ with\n",
      "$$P(B) = \\sum_{i=1}^k P(B|A_i)P(A_i).$$\n",
      "\n",
      "\n",
      "**Interpretation.** Suppose the events $A_1,\\dots , A_k$ denote an exhaustive and exclusive set of \"diseases\" for a living or mechanical system (including \"normal state\"), and the event $B$ denotes a \"syndrome\", i.e., a set of symptoms.\n",
      "\n",
      "In many cases, some empirical information about $P(B|A_i)$ and $P(A_i)$ can be obtained. Bayes\u2019 Theorem can then be used to invert these probabilities to estimate $P(A_i|B)$, a quantity of primary diagnostic interest. $P(A_i)$ is called the *prior probability *of event $A_i$, and $P(A_i|B)$ is called the posterior probability of event $A$ given event (or: information) $B$. In objectivistic, \"empirical\" Bayesian probability theory, $P(A_i)$ is interpreted as the \u2018prevalence\u2019, i.e., the frequency of occurrence, of disease $A_i$ in some, suitable, reference population. This prevalence can be estimated, at least in principle, by drawing a sample. In the subjectivistic interpretation of Bayesian probability theory, $P(A_i)$ is considered as the (personal) prior strength of belief attached to the proposition that the system under investigation is affected by the disease $A_i$. Naturally, objectivists and either rationalist or personalist Bayesians hold different meta-statistical viewpoints, not on the mathematical correctness of Bayes\u2019 theorem, but on the empirical situations the theorem can be fruitfully applied. In the first case, attention is restricted to the properties of a physical system in repetitive situations, and in the second case to our (declared) knowledge thereof or, even more generally, of uncertain states of affairs. \n",
      "\n",
      "An application of Bayes\u2019 theorem is given in the following exercise.\n",
      "\n",
      "**Exercise:** Suppose a series of plasma discharges of a certain \u2018standard type\u2019 are being produced, e.g., deuterium plasmas heated by deuterium neu- tral beams at a certain plasma current, magnetic field and density, with standardised wall conditioning. Because not all experimental parameters are completely known, even in this homogeneous class of discharges, some events still occur (seemingly) in a haphazard fashion, but we have been in a position to record what happened in a number of such similar discharges. The plasma can suffer from a disruption ($D = +$) or stay disruption-free ($D = \u2212$). A monitor (i.e., warning) signal exists, which is in this case the ratio between two spectroscopically measured impurity signals, for instance the ratio of carbon to oxygen concentration, [C]/[O]. For simplicity, we consider this ratio to be discretised into a small number of levels $m = 1, . . . , k$. From previous experience, a disruption occurred, on average for this type of discharges, in $1/2$ of all cases.\n",
      "Estimate $P(D = +|M = m)$, i.e., the posterior probability of a disruption, given that the level of monitor signal is m, on the basis of the (additional) information given in the table belwo, based on a small but representative subset of discharges. \n",
      "\n",
      "**Hint:** Use the formula $P(D = +|M = m) =\n",
      "P(M =m|D=+)P(D=+) . (1.19) P(M = m|D = +)P(D = +) + P(M = m|D = \u2212)P(D = \u2212)$\n",
      "What is $P(D = +|M = m)$ if the \u2018prior probability\u2019 $P(D = +) = 1/2$ is replaced by $P(D = +) = 1/3$, as is suggested by the small subset of the data, displayed in the table below? Check your answer by an argument not based on Bayes\u2019 theorem.\n",
      "\n",
      "\n",
      "**Table: disruption frequencies** \n",
      "<table>\n",
      "<tr> D|m 0 1 2 3 total </tr>\n",
      "<tr> + 4 8 12 8 32  </tr>\n",
      "<tr> \u2013 8 2 4 2 16 </tr>\n",
      "</table>\n",
      "\n",
      "For didactic purposes, fictitious numbers have been used."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Random Variables "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Definition:** A *random variable* is a (measurable) function $X : \\Omega \\rightarrow \\mathbb R$, where $(\\Omega, \\mathcal B(\\Omega), P)$ is a general probability space.\n",
      "\n",
      "Such a function induces from the given probability measure $P$ on $\\Omega$ a new probability measure $P_X$ on $\\mathbb R$: \n",
      "\n",
      "$$P_X(A) := P(X^{-1}(A)),$$\n",
      "\n",
      "for each interval $A \\subset \\mathbb R$, and whence for each (measurable) subset in $\\mathbb R$. Mathematically, $P(X^{-1}(A))$ is defined as\n",
      "\n",
      "$$P(\\{\\omega:\\: X(\\omega) \\in A\\}),$$\n",
      "\n",
      "which is frequently abbreviated as $P(X \\in  A)$. $P_X$ (and if the context is clear, \u2018par abus de langage\u2019, even $P$ ) is also called the probability distribution associated with the random variable $X$.\n",
      "\n",
      "**Example:** $\\Omega = \\{1, 2, 3, 4, 5, 6\\}^n$  is the set of outcomes when throwing $n$ dice (subsequently or in one throw if they are identified by numbers $1,\\dots,n$), or when throwing the same die $n$ times. $X_1$ is the number of times a $3$ turned up, $X_2$ is the total number of dots that turned up, $X_3 = X_2/n$ is the average number of dots that turned up. $X_1$ defines a probability measure on $\\{0,1,2,\\dots,n\\}$, which can also be viewed as a probability measure on $\\mathbb R$. As we will see later in this chapter, this probability measure can be\n",
      "easily characterised, and is called the *binomial distribution*. The probability distributions induced by $X_2$ and $X_3$ are more complicated. The important point is here to see the difference and the connection between $(\\Omega, \\mathcal B(\\Omega), P)$ and $(\\mathbb R, \\mathcal B(\\mathbb R), P_X )$.\n",
      "\n",
      "**Definition** The *distribution function* $F$ of a random variable $X$ is the function $F : \\mathbb R \\rightarrow [0,1]$ defined by $F(x) = P(X \u2264 x)$, with $x \\in \\mathbb R$.\n",
      "\n",
      "**Borel-Lebesgue Theorem:** \n",
      "\n",
      "1. A distribution function has the following properties: $F (-\\infty) = 0$, $F (\\infty) = 1$, $F$ is monotonic (hence, non-decreasing).\n",
      "\n",
      "1. Each right-continuous function $F$ with the properties (1) defines a probability measure by $P(a < X \u2264 b) = F(b) \u2212 F(a)$.\n",
      "\n",
      "Although measure theory a la Bourbaki has been conceived to comprise both cases in one framework, in practice, it is useful to distinguish between discrete and continuous probability distributions:\n",
      "\n",
      "**Definition:** A probability distribution is *discrete* if $X$ can assume at most countably many values, i.e. if its distribution function $F$ is a step function with at most countably many jumps.\n",
      "\n",
      "**Definition:** A probability distribution $F$ is called continuous if $F$ is \u2018absolutely continuous\u2019, i.e., practically speaking, if $F$ is continuous and, possibly except for (at most) countably many isolated points, possesses a continuous derivative $F\u2032 = f$ (with respect to its argument $x$).\n",
      "\n",
      "In this case $P(X = x_i) = F(x_i^+ )\u2212 F(x_i^-)$ is just the jump of $F$ at $x_i$, $i = 1,2,\\dots$. We frequently abbreviate $P(X = x_i)$ by $p_i$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Expectation, variance, and quantile"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Definition:** For a *continuous random variable* with *density function* $f$ one defines\n",
      "the expectation value of the associated random variable $X$ by\n",
      "\n",
      "$$E(X) = \\int_\\Omega X(\\omega)dP(\\omega) = \\int_{-\\infty}^{\\infty} x f(x) dx,$$\n",
      "and the variance of X as the expected value of the random variable $(X - E(X))^2$: i.e.\n",
      "\n",
      "$$var(X)  = \\int_{-\\infty}^\\infty (x- E(X))^2 f(x) dx.$$\n",
      "\n",
      "\n",
      "**Definition:** For a *discrete random variable* $X$ with *frequency function* $f(x_i) = p_i$ are $x_1, x_2, \\dots$ are the possible values that $X$ can take, the expectation of $X$ is defined as\n",
      "$$E(X) = \\sum_{i=1}^\\infty x_i f(x_i)$$ \n",
      "and it variance is the expectation of the discrete random variable $(X - E(X))^2$: i.e.\n",
      "\n",
      "$$ var(X) = \\sum_{i=1}^\\infty (x_i - E(X))^2 f(x_i).$$\n",
      "\n",
      "**Notation:** It's common to use the following symbols to denote interchangably the expectation and the variance of a random variable:\n",
      "\n",
      "* $\\mu = \\mu_X = E(X)$ \n",
      "* $\\sigma^2 = SD(X)^2 = var(X)$\n",
      "\n",
      "The square root of the variance is called the **standard deviation:**\n",
      "\n",
      "$$\\sigma = SD(X) = \\sqrt{var(X)}.$$\n",
      "\n",
      "**Exercise:** Show that $var(X) = E(X^2) - \\mu^2$ for both continous and discrete random variables.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Definition:** For a continuous distribution, an $\\alpha$-quantile is defined as\n",
      "$F^{-1}(\\alpha)$, where $\\alpha$ is some fraction between 0 and 1. For a discrete distribution, $F(x)$ is a step function and a difficulty arises. One way to define a (unique) inverse, is\n",
      "$$F^{-1}(\\alpha) = \\inf_x \\{ F(x) \u2265 \\alpha \\}.$$ This definition has the feature that the median\n",
      "of two observations, ${x1,x2}$, is equal to the smaller of the two observations.\n",
      "\n",
      "A 50% quantile, $F^{\u22121}(1/2)$, is usually called median. \n",
      "For $\\alpha = 1/4, 1/2, 3/4$, the quantity $F^{-1}(\\alpha)$ is called (lower, middle, upper) quartile, whereas for $\\alpha = j/10$ the term $j^{th}$ decile is used ($j = 1,2,...,9$)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Independence of random variables"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Definition:** The **simultaneous distribution** of two random variables $X$ and $Y$ is characterised by a probability measure $P_{X,Y}$ on $\\mathbb R^2$ where \n",
      "\n",
      "$$P_{X,Y}(A) = P(\\{\\omega : (X(\\omega),Y(\\omega) \\in A\\})$$ \n",
      "\n",
      "denotes, for each measurable subset $A \\subset \\mathbb R$, the\n",
      "probability that the random point $(X, Y)$ falls in $A$.\n",
      "\n",
      "**Definition:** The **joint distribution function** of $X$ and $Y$ is \n",
      "\n",
      "$$F(x,y) = P(\\{X \u2264 x,Y \u2264y\\}).$$\n",
      "\n",
      "\n",
      "**Definition:** The **marginal distributions** of $X$ and $Y$ are given by the distribution functions \n",
      "\n",
      "$$F_X(x) = P(\\{X \u2264 x,Y <\u221e\\}) \\quad \\textrm{and}\\quad F_Y(y) = P(\\{X<\u221e,Y \u2264y\\}),$$\n",
      "\n",
      "\n",
      "respectively.\n",
      "\n",
      "**Definition:** Two random variables $X$ and $Y$ are called *independent* if \n",
      "\n",
      "$$P(X \\in A\\, \\textrm{and} \\, Y \\in B) = P(X \\in A)P(Y \\in B),$$\n",
      "\n",
      "for all non-pathological subsets $A$ and $B$. \n",
      "\n",
      "**Theorem:** Two random variables are independent if and only if \n",
      "\n",
      "$$F (x, y) = F (x)F (y),$$\n",
      "\n",
      "and, for continuous distributions, if and only if \n",
      "\n",
      "$$f (x, y) = f_X (x)f_Y (y).$$\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Correlation of random variables"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Definition** Let $X$ and $Y$ have a joint distribution with expectations $\\mu_X = E(X)$ and $\\mu_Y = E(Y)$. The *covariance* between $X$ and $Y$ is\n",
      "\n",
      "$$cov(X,Y)= E(X\u2212\\mu_X)(Y\u2212\\mu_Y).$$\n",
      "\n",
      "\n",
      "**Definition:** The *correlation coefficient* between $X$ and $Y$ is\n",
      "\n",
      "$$\\rho(X,Y)= \\frac{cov(X,Y)}{\\sqrt{var(X)var(Y)}}.$$\n",
      "\n",
      "\n",
      "**Correlation Theorem:** If $X$ and $Y$ are independent then they are uncorrelated (i.e.,\n",
      "$\\rho(X, Y ) = 0$). (The converse does not hold true.)\n",
      "\n",
      "\n",
      "**Exercise:** Let $Y = X^2$, where $X$ has a symmetric distribution with mean zero. (Hence, its skewness is also zero.) Prove that $\\rho(X, Y ) = 0$, while obviously $X$ and $Y$ are not independent.\n",
      "\n",
      "\n",
      "**Linearity Theorem:** For two random variables $X$ and $Y$ (whether they are inde- pendent or not )\n",
      "\n",
      "$$E(aX + bY ) = aE(X) + bE(Y).$$\n",
      "\n",
      "Moreover, if $X$ and $Y$ are independent, then\n",
      "$$var (X \\pm Y ) = var (X ) + var (Y ) .$$\n",
      "In general,\n",
      "\n",
      "$$var (aX \\pm bY ) = a2var (X) + b2var (Y ) \\pm 2 a b cov (X, Y ) .$$\n",
      "\n",
      "**Exercise:**  Prove that $E(aX + bY ) = aE(X) + bE(Y )$. Recall that\n",
      "$E(aX+bY)=\\int_{-\\infty}^\\infty(ax+by)f(x,y)dxdy$.\n",
      "Prove also the second part of the linearity theorem."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Analogy with mechanics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The mechanical equivalent of a probability distribution is a mass distribution. This entails the following correspondence:\n",
      "\n",
      "probability $\\longrightarrow$ mechanics\n",
      "\n",
      "expectation value $\\mu$ $\\longrightarrow$ centre of gravity\n",
      "\n",
      "second moment $E(X^2)$ $\\longrightarrow$ moment of inertia w.r.t. the origin\n",
      "\n",
      "variance $var(X)$ $\\longrightarrow$ moment of inertial w.r.t. $\\mu$\n",
      "\n",
      "$E(X^2) = var (X) + \\mu^2$ $\\longrightarrow$ Steiner's rule\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Analogy with geometry"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One can show, under weak regularity conditions, that\n",
      "\n",
      "1. $cov (X, Y ) = cov (Y, X)$\n",
      "1. $cov (X, aY1 + bY2) = a cov (X, Y1) + b cov (X, Y2)$\n",
      "1. $cov (X, Y ) \u2264 (var (X) var (Y ))^{1/2}$\n",
      "1. $var (X ) = 0 \\iff X$ is deterministic\n",
      "\n",
      "Hence, $cov (X, Y )$ has similar properties as an **inner product**. (In fact, it is the inner product between $X \u2212 E(X)$ and $Y \u2212 E(Y)$.) Using the inner product notation \n",
      "\n",
      "$$\\langle X, Y \\rangle = |\\|X \\| \\|Y \\| \\cos \\phi,$$\n",
      "where $\\phi$ is the angle between the vectors $X$ and $Y$ , one can relate:\n",
      "\n",
      "$cov(X,Y)$ $\\longrightarrow$  $\\langle X, Y \\rangle$\n",
      "\n",
      "$var (X )$ $\\longrightarrow$  $\\|X \\|^2$\n",
      "\n",
      "\n",
      "$\\rho(X, Y )$ $\\longrightarrow$  $\\cos \\phi$\n",
      "\n",
      "Note, however, that the space of random variables is infinite-dimensional. The state of affairs is more precisely expressed by:\n",
      "\n",
      "$$\\{X |X : \\Omega\\rightarrow \\mathbb R, var (X ) < \\infty \\}$$\n",
      "\n",
      "is a Hilbert space with respect to the inner product \n",
      "\n",
      "$$\\langle X,Y \\rangle = cov(X,Y),$$\n",
      "\n",
      "if one considers equivalence classes of random variables that differ only a deterministic constant. This feature is useful to keep in the back of one\u2019s mind when making standard manipulations in practical calculations.\n",
      "\n",
      "\n",
      "**Exercise:** Prove that $var(X)=var(X+a)$ and $cov(X,Y)=cov(X+ a, Y + b)$ for all real numbers $a$ and $b$, and relate this to the sentence about the Hilbert space.\n",
      "\n",
      "\n",
      "**Exercise:** Extend the mechanical analogue to two-dimensional distributions."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The Central Limit Theorem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Theorem** Let $X_1$, $X_2$, $\\dots$ be independent random variables having the same (arbitrary) probability distribution with (finite) expectation value $\\mu$ and variance $\\sigma^2$. Then, the distribution of \n",
      "\n",
      "$$Z_n = (X_1 + \u00b7 \u00b7 \u00b7 + X_n \u2212 n\\mu)/(\\sigma\\sqrt{n})$$\n",
      "\n",
      "tends for $n\\longrightarrow \\infty$ to the standard normal, i.e.,\n",
      "the distribution function of $Z_n$ tends to \n",
      "\n",
      "$$\\Phi(z) = \\int_{-\\infty}^z \\frac1{\\sqrt{2\\pi}}dt.$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}